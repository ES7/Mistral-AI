{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7f6058-d0d1-44af-b43a-cb4b06df03d8",
   "metadata": {},
   "source": [
    "# Prompting Capabilities \n",
    "Prompt the mistral models via API calls and perform various tasks like classification information extraction, personalization and summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa37d97-10e4-425d-b852-15bd043662b9",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# !pip install mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f6a5640-203a-4e7a-bc4c-4bfe78da4099",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from helper import load_mistral_api_key\n",
    "load_mistral_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e030d9c2-1ecb-4bf0-864d-9f96b41bd016",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! I can assist you with a variety of tasks. I can answer questions, provide information, set reminders, manage your schedule, send messages, and much more. I can also help you learn new things, like a new language or a new skill. How can I assist you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helper import mistral\n",
    "mistral(\"hello, what can you do?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98a29ef-4764-40b7-aed8-0e5d0502f985",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d71e9d1c-ca45-4d19-882c-07e077ea19ad",
   "metadata": {
    "height": 642
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    You are a bank customer service bot. \n",
    "    Your task is to assess customer intent and categorize customer \n",
    "    inquiry after <<<>>> into one of the following predefined categories:\n",
    "    \n",
    "    card arrival\n",
    "    change pin\n",
    "    exchange rate\n",
    "    country support \n",
    "    cancel transfer\n",
    "    charge dispute\n",
    "    \n",
    "    If the text doesn't fit into any of the above categories, \n",
    "    classify it as:\n",
    "    customer service\n",
    "    \n",
    "    You will only respond with the predefined category. \n",
    "    Do not provide explanations or notes. \n",
    "    \n",
    "    ###\n",
    "    Here are some examples:\n",
    "    \n",
    "    Inquiry: How do I know if I will get my card, or if it is lost? I am concerned about the delivery process and would like to ensure that I will receive my card as expected. Could you please provide information about the tracking process for my card, or confirm if there are any indicators to identify if the card has been lost during delivery?\n",
    "    Category: card arrival\n",
    "    Inquiry: I am planning an international trip to Paris and would like to inquire about the current exchange rates for Euros as well as any associated fees for foreign transactions.\n",
    "    Category: exchange rate \n",
    "    Inquiry: What countries are getting support? I will be traveling and living abroad for an extended period of time, specifically in France and Germany, and would appreciate any information regarding compatibility and functionality in these regions.\n",
    "    Category: country support\n",
    "    Inquiry: Can I get help starting my computer? I am having difficulty starting my computer, and would appreciate your expertise in helping me troubleshoot the issue. \n",
    "    Category: customer service\n",
    "    ###\n",
    "    \n",
    "    <<<\n",
    "    Inquiry: {inquiry}\n",
    "    >>>\n",
    "    Category:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d367f-cc2c-4857-abd5-d1f6a545ebc0",
   "metadata": {},
   "source": [
    "In the above prompt our task is to assess customer intent and categorize customer inquiry. We have a list of predefined categories. If the text doesn't fit in any of the categories, classify it as customer service. In the above prompt we first assign a **role play** to the model as a `bank customer service bot` this adds personal context to the model. Next we used **few shot learning** where we give some exapmples in the prompts. This can improve model's performance, especially when the task is difficult or when we want the model to respond in a specific manner.\n",
    "We use demminetors like '###' and '<<<' to specify the boundary between diferent sections of the text. In our example, we use the '###' to indicate examples and '<<<' to indicate customer inquiry. Finally in a case when the model is **verbose**, we can add: \"do not provide explanations or notes\", to make sure the output is concise.\n",
    "\n",
    "#### If we want to make sure that our prompt doesn't have any grammetical mistake, ask Mistral to check the spelling and grammar of the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "465a2ecd-e542-4863-96dc-29786c003799",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "response = mistral(f\"Please correct the spelling and grammar of \\\n",
    "this prompt and return a text that is the same prompt,\\\n",
    "with the spelling and grammar fixed: {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "476f0db5-ba51-43ca-a4c4-dc2b072291ba",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a bank customer service bot.\n",
      "Your task is to assess customer intent and categorize the customer inquiry following the \"<<< >>>\" into one of the following predefined categories:\n",
      "\n",
      "card arrival\n",
      "change pin\n",
      "exchange rate\n",
      "country support\n",
      "cancel transfer\n",
      "charge dispute\n",
      "\n",
      "If the text doesn't fit into any of the above categories, classify it as:\n",
      "customer service\n",
      "\n",
      "You will only respond with the predefined category. Do not provide explanations or notes.\n",
      "\n",
      "###\n",
      "Here are some examples:\n",
      "\n",
      "Inquiry: How do I know if I will get my card, or if it is lost? I am concerned about the delivery process and would like to ensure that I will receive my card as expected. Could you please provide information about the tracking process for my card, or confirm if there are any indicators to identify if the card has been lost during delivery?\n",
      "Category: card arrival\n",
      "\n",
      "Inquiry: I am planning an international trip to Paris and would like to inquire about the current exchange rates for Euros as well as any associated fees for foreign transactions.\n",
      "Category: exchange rate\n",
      "\n",
      "Inquiry: What countries are getting support? I will be traveling and living abroad for an extended period of time, specifically in France and Germany, and would appreciate any information regarding compatibility and functionality in these regions.\n",
      "Category: country support\n",
      "\n",
      "Inquiry: Can I get help starting my computer? I am having difficulty starting my computer, and would appreciate your expertise in helping me troubleshoot the issue.\n",
      "Category: customer service\n",
      "###\n",
      "\n",
      "<<<\n",
      "Inquiry: {inquiry}\n",
      ">>>\n",
      "Category:\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27bbf68-a0bc-4e8a-bb6b-8ae20f3f2e2d",
   "metadata": {},
   "source": [
    "Now using the corrected prompt let's try out the model by passing the inquiry which will enter in `{inquiry}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4625fdc0-c6ef-4dcf-bbc0-d250a0ed277c",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'country support'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral(\n",
    "    response.format(\n",
    "        inquiry=\"I am inquiring about the availability of your cards in the EU\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4252e995-f3fc-4e62-abdb-3e367df55cbe",
   "metadata": {},
   "source": [
    "## Information Extraction with JSON Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6741a613-5b1b-4fb1-9843-1c5737f36cd5",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "medical_notes = \"\"\"\n",
    "A 60-year-old male patient, Mr. Johnson, presented with symptoms\n",
    "of increased thirst, frequent urination, fatigue, and unexplained\n",
    "weight loss. Upon evaluation, he was diagnosed with diabetes,\n",
    "confirmed by elevated blood sugar levels. Mr. Johnson's weight\n",
    "is 210 lbs. He has been prescribed Metformin to be taken twice daily\n",
    "with meals. It was noted during the consultation that the patient is\n",
    "a current smoker. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd839f-a3d4-4a15-b5ac-7373e2a59d92",
   "metadata": {},
   "source": [
    "The above text is of some medical information and we would like to extract some information from this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14c2bd7a-48c9-4c0b-8a4a-049a43045805",
   "metadata": {
    "height": 472
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Extract information from the following medical notes:\n",
    "{medical_notes}\n",
    "\n",
    "Return json format with the following JSON schema: \n",
    "\n",
    "{{\n",
    "        \"age\": {{\n",
    "            \"type\": \"integer\"\n",
    "        }},\n",
    "        \"gender\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"male\", \"female\", \"other\"]\n",
    "        }},\n",
    "        \"diagnosis\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"migraine\", \"diabetes\", \"arthritis\", \"acne\"]\n",
    "        }},\n",
    "        \"weight\": {{\n",
    "            \"type\": \"integer\"\n",
    "        }},\n",
    "        \"smoking\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"yes\", \"no\"]\n",
    "        }}\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552b413-0296-4fa6-b7d3-f1d3d106f763",
   "metadata": {},
   "source": [
    "In this prompt, we provide the medical notes and ask the model to return JSON format with the following JSON schema, where we define what we want to extract, the type of this variable and the list of output options. So for diagnosis, the model should output one of the four options provided in diagnosis section.\n",
    "In this promt we explicitly ask to return JSON format. It is important to ask for the JSON format when we enable the JSON mode. Another strategy we use here, is that we define thte JSON schema. We use this JSON schema in the prompt to ensure the consistency and structure of the JSON output. Note that if we don't have the is_JSON=True the output may still be a JSON format, but it is recommended to enable the JSON mode to return a reliable JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "951a0b58-aae5-45e2-8496-714b884f16b6",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "        \"age\": 60,\n",
      "        \"gender\": \"male\",\n",
      "        \"diagnosis\": \"diabetes\",\n",
      "        \"weight\": 210,\n",
      "        \"smoking\": \"yes\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = mistral(prompt, is_json=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a9cb95-bb08-4929-b16c-eb19877f3c01",
   "metadata": {},
   "source": [
    "## Personalization\n",
    "LLMs are really good at personalization tasks, let's see how the model can create personalized email responses to address customer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cf048b4-3e33-4753-af97-25b73c51ee6a",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Dear mortgage lender, \n",
    "\n",
    "What's your 30-year fixed-rate APR, how is it compared to the 15-year \n",
    "fixed rate?\n",
    "\n",
    "Regards,\n",
    "Anna\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d99e871-522d-4f3e-8b0f-dd8fff65dfec",
   "metadata": {},
   "source": [
    "The above text is an email where the customer 'Anna' is asking the mortgage lender about the mortgage rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36de7c1e-60c2-4f35-a51a-115b12d65bb6",
   "metadata": {
    "height": 404
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "\n",
    "You are a mortgage lender customer service bot, and your task is to \n",
    "create personalized email responses to address customer questions.\n",
    "Answer the customer's inquiry using the provided facts below. Ensure \n",
    "that your response is clear, concise, and directly addresses the \n",
    "customer's question. Address the customer in a friendly and \n",
    "professional manner. Sign the email with \"Lender Customer Support.\"   \n",
    "      \n",
    "# Facts\n",
    "30-year fixed-rate: interest rate 6.403%, APR 6.484%\n",
    "20-year fixed-rate: interest rate 6.329%, APR 6.429%\n",
    "15-year fixed-rate: interest rate 5.705%, APR 5.848%\n",
    "10-year fixed-rate: interest rate 5.500%, APR 5.720%\n",
    "7-year ARM: interest rate 7.011%, APR 7.660%\n",
    "5-year ARM: interest rate 6.880%, APR 7.754%\n",
    "3-year ARM: interest rate 6.125%, APR 7.204%\n",
    "30-year fixed-rate FHA: interest rate 5.527%, APR 6.316%\n",
    "30-year fixed-rate VA: interest rate 5.684%, APR 6.062%\n",
    "\n",
    "# Email\n",
    "{email}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc1cc84-94c2-47c1-a12b-523550834193",
   "metadata": {},
   "source": [
    "In the above prompt we have provided some numbers about the interest rates in the prompts. We use the string format to add the actual email content to this email variable at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaf50774-0f91-4e0e-86c9-1525f6045ebb",
   "metadata": {
    "height": 47,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Re: Inquiry on 30-Year and 15-Year Fixed-Rate Loans\n",
      "\n",
      "Dear Anna,\n",
      "\n",
      "Thank you for reaching out to us with your inquiry. I'm delighted to help!\n",
      "\n",
      "Our current 30-year fixed-rate mortgage carries an APR of 6.484%. When comparing this with our 15-year fixed-rate mortgage, you'll find that the APR for the 15-year term is lower at 5.848%. This difference is due to the shorter loan term of the 15-year fixed-rate mortgage, which translates to less interest paid over time.\n",
      "\n",
      "Please let me know if you have any further questions or if you would like additional information on our loan options.\n",
      "\n",
      "Best regards,\n",
      "Lender Customer Support\n"
     ]
    }
   ],
   "source": [
    "response = mistral(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfaad2c-411e-4221-80f0-7acd21ba398c",
   "metadata": {},
   "source": [
    "We can see that we get a personalized email to Anna answering her questions based on the facts provided. With this kind of prompt, we can easily create our own customer service bot. Answer questions about our product. It is important to use clear and concise language when presenting these facts or your product information. This can help the model to provide accurate and quick responses to customer queries.\n",
    "\n",
    "## Summarization\n",
    "\n",
    "- We'll use this [article](https://www.deeplearning.ai/the-batch/mistral-enhances-ai-landscape-in-europe-with-microsoft-partnership-and-new-language-models) from The Batch. Summarization is a common task for LLMs, and Mistral model can do a really good job as summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bbab492-1d18-4832-86a9-2fba645e0e52",
   "metadata": {
    "height": 336
   },
   "outputs": [],
   "source": [
    "newsletter = \"\"\"\n",
    "European AI champion Mistral AI unveiled new large language models and formed an alliance with Microsoft. \n",
    "\n",
    "What’s new: Mistral AI introduced two closed models, Mistral Large and Mistral Small (joining Mistral Medium, which debuted quietly late last year). Microsoft invested $16.3 million in the French startup, and it agreed to distribute Mistral Large on its Azure platform and let Mistral AI use Azure computing infrastructure. Mistral AI makes the new models available to try for free here and to use on its La Plateforme and via custom deployments.\n",
    "\n",
    "Model specs: The new models’ parameter counts, architectures, and training methods are undisclosed. Like the earlier, open source Mistral 7B and Mixtral 8x7B, they can process 32,000 tokens of input context. \n",
    "\n",
    "Mistral Large achieved 81.2 percent on the MMLU benchmark, outperforming Anthropic’s Claude 2, Google’s Gemini Pro, and Meta’s Llama 2 70B, though falling short of GPT-4. Mistral Small, which is optimized for latency and cost, achieved 72.2 percent on MMLU.\n",
    "Both models are fluent in French, German, Spanish, and Italian. They’re trained for function calling and JSON-format output.\n",
    "Microsoft’s investment in Mistral AI is significant but tiny compared to its $13 billion stake in OpenAI and Google and Amazon’s investments in Anthropic, which amount to $2 billion and $4 billion respectively.\n",
    "Mistral AI and Microsoft will collaborate to train bespoke models for customers including European governments.\n",
    "Behind the news: Mistral AI was founded in early 2023 by engineers from Google and Meta. The French government has touted the company as a home-grown competitor to U.S.-based leaders like OpenAI. France’s representatives in the European Commission argued on Mistral’s behalf to loosen the European Union’s AI Act oversight on powerful AI models. \n",
    "\n",
    "Yes, but: Mistral AI’s partnership with Microsoft has divided European lawmakers and regulators. The European Commission, which already was investigating Microsoft’s agreement with OpenAI for potential breaches of antitrust law, plans to investigate the new partnership as well. Members of President Emmanuel Macron’s Renaissance party criticized the deal’s potential to give a U.S. company access to European users’ data. However, other French lawmakers support the relationship.\n",
    "\n",
    "Why it matters: The partnership between Mistral AI and Microsoft gives the startup crucial processing power for training large models and greater access to potential customers around the world. It gives the tech giant greater access to the European market. And it gives Azure customers access to a high-performance model that’s tailored to Europe’s unique regulatory environment.\n",
    "\n",
    "We’re thinking: Mistral AI has made impressive progress in a short time, especially relative to the resources at its disposal as a startup. Its partnership with a leading hyperscaler is a sign of the tremendous processing and distribution power that remains concentrated in the large, U.S.-headquartered cloud companies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaede63d-7392-4f1c-8a87-507ee31fe246",
   "metadata": {
    "height": 472
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a commentator. Your task is to write a report on a newsletter. \n",
    "When presented with the newsletter, come up with interesting questions to ask,\n",
    "and answer each question. \n",
    "Afterward, combine all the information and write a report in the markdown\n",
    "format. \n",
    "\n",
    "# Newsletter: \n",
    "{newsletter}\n",
    "\n",
    "# Instructions: \n",
    "## Summarize:\n",
    "In clear and concise language, summarize the key points and themes \n",
    "presented in the newsletter.\n",
    "\n",
    "## Interesting Questions: \n",
    "Generate three distinct and thought-provoking questions that can be \n",
    "asked about the content of the newsletter. For each question:\n",
    "- After \"Q: \", describe the problem \n",
    "- After \"A: \", provide a detailed explanation of the problem addressed \n",
    "in the question.\n",
    "- Enclose the ultimate answer in <>.\n",
    "\n",
    "## Write a analysis report\n",
    "Using the summary and the answers to the interesting questions, \n",
    "create a comprehensive report in Markdown format. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb6fb24-dbb2-4e3a-ba58-21d7825abf44",
   "metadata": {},
   "source": [
    "We have given the `{newsletter}` and provided some instructions followed by some questions to be answered and lastly to write an analysis report of the article. These instructions help the model to think in each step and generate a more comprehensive report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5505b0a5-411b-4804-aaef-ccecfa3d07be",
   "metadata": {
    "height": 47,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Summary\n",
      "European AI startup Mistral AI has introduced two new large language models, Mistral Large and Mistral Small, and formed a strategic alliance with Microsoft. The partnership involves Microsoft investing in Mistral AI, distributing Mistral Large on its Azure platform, and providing Azure computing infrastructure. Mistral AI's new models outperform several competitors on the MMLU benchmark, and both models are fluent in multiple European languages. The partnership has received mixed reactions from European lawmakers and regulators, with some expressing concerns over data access and others supporting the relationship.\n",
      "\n",
      "# Interesting Questions\n",
      "## Q: What are the specific advantages of Mistral AI's new language models compared to their competitors?\n",
      "A: Mistral AI's new language models, Mistral Large and Mistral Small, have demonstrated superior performance on the MMLU benchmark compared to Anthropic's Claude 2, Google’s Gemini Pro, and Meta’s Llama 2 70B. However, the exact advantages of these models over their competitors are not explicitly stated in the newsletter. It could be due to the undisclosed parameter counts, architectures, and training methods. Additionally, Mistral Large and Mistral Small are trained to be fluent in multiple European languages, which may give them an edge in certain applications.\n",
      "\n",
      "## Q: How does the partnership between Mistral AI and Microsoft benefit each party?\n",
      "A: The partnership between Mistral AI and Microsoft provides several advantages for both parties. For Mistral AI, it offers access to Microsoft's processing power for training large models and a wider customer base through the Azure platform. For Microsoft, it expands its presence in the European market and provides Azure customers with access to a high-performance model tailored to Europe's unique regulatory environment. Furthermore, the collaboration between the two companies allows them to develop bespoke models for European governments and other clients.\n",
      "\n",
      "## Q: What are the concerns raised by European lawmakers and regulators about the partnership between Mistral AI and Microsoft?\n",
      "A: European lawmakers and regulators have expressed concerns about the partnership between Mistral AI and Microsoft due to potential data access issues. Some members of President Emmanuel Macron's Renaissance party criticized the deal's potential to give a U.S. company access to European users' data. This concern is compounded by the fact that Microsoft is already under investigation by the European Commission for potential antitrust violations related to its agreement with OpenAI. However, other French lawmakers support the relationship, indicating a split within the European political landscape regarding the partnership.\n",
      "\n",
      "# Analysis Report\n",
      "In the latest newsletter, European AI startup Mistral AI unveiled two new large language models, Mistral Large and Mistral Small, and formed a strategic alliance with Microsoft. This partnership provides Mistral AI with crucial processing power for training large models and greater access to potential customers around the world. For Microsoft, it expands its presence in the European market and offers Azure customers access to a high-performance model tailored to Europe's unique regulatory environment.\n",
      "\n",
      "However, the partnership has raised concerns among European lawmakers and regulators due to potential data access issues. Some members of President Emmanuel Macron's Renaissance party have criticized the deal's potential to give a U.S. company access to European users' data. This concern is compounded by the fact that Microsoft is already under investigation by the European Commission for potential antitrust violations related to its agreement with OpenAI.\n",
      "\n",
      "Despite these concerns, Mistral AI's new language models have demonstrated superior performance on the MMLU benchmark compared to several competitors. The models are also trained to be fluent in multiple European languages, which may provide a competitive advantage in certain applications. Furthermore, the collaboration between Mistral AI and Microsoft allows them to develop bespoke models for European governments and other clients.\n",
      "\n",
      "Overall, the partnership between Mistral AI and Microsoft represents a significant development in the European AI landscape. It highlights the tremendous processing and distribution power that remains concentrated in large, U.S.-headquartered cloud companies. However, it also underscores the need for careful consideration of data privacy and regulatory concerns in such partnerships.\n"
     ]
    }
   ],
   "source": [
    "response = mistral(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed88147-8d43-4207-b45b-06543371f913",
   "metadata": {},
   "source": [
    "## The Mistral Python client\n",
    "- Below is the helper function that we imported from helper.py and used in this notebook.\n",
    "- For more details, check out the [Mistral AI API documentation](https://docs.mistral.ai/api/)\n",
    "- To get our own Mistral AI API key to use on our own, we can create an account and go to the [console](https://console.mistral.ai/) to subscribe and create an API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27ef7a7f-ebd6-49c3-8f91-5f5d284edf17",
   "metadata": {
    "height": 353
   },
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage   \n",
    "\n",
    "def mistral(user_message, \n",
    "            model=\"mistral-small-latest\",\n",
    "            is_json=False):\n",
    "    client = MistralClient(api_key=os.getenv(\"MISTRAL_API_KEY\"))\n",
    "    messages = [ChatMessage(role=\"user\", content=user_message)]\n",
    "\n",
    "    if is_json:\n",
    "        chat_response = client.chat(\n",
    "            model=model, \n",
    "            messages=messages,\n",
    "            response_format={\"type\": \"json_object\"})\n",
    "    else:\n",
    "        chat_response = client.chat(\n",
    "            model=model, \n",
    "            messages=messages)\n",
    "        \n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd279d2-d4cd-4465-9d65-8143a16c4bca",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
