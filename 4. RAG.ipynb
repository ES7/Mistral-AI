{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d67a4729-cd2f-47e7-a4f6-f84a5677414f",
   "metadata": {},
   "source": [
    "# Basic RAG (Retrieval Augmented Generation)\n",
    "Retrieval Augmented Generation is an AI framework that combines the capabilities of LLM and information retrieval systems. It is useful to answer questions or generate content leveraging external knowledge.\n",
    "Why do we need RAG? LLMs can face a lot of challenges, it doesn’t have access to our internal document, it doesn’t have the most up to date information and it can hallucinate. One of the potential solutions for these problems is RAG.\n",
    "When users ask a question about an internal document or a knowledge base, we retrieve relevant information from the knowledge base, where all the text embeddings are stored in a vector store, this step is called retrieval. Then in a prompt we include both the user query and the relevant information, so that our model can generate output based on the relevant context, this step is called generation.  \n",
    "<img src=\"RAG.png\" width=\"600\" height=\"400\"><br>\n",
    "```python\n",
    "! pip install faiss-cpu \"mistralai>=0.1.2\"\n",
    "from helper import load_mistral_api_key\n",
    "api_key, dlai_endpoint = load_mistral_api_key(ret_key=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b880d1ed-3db0-45a1-807e-1b47e9ce1320",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# ! pip install faiss-cpu \"mistralai>=0.1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b100be-c2cf-4e07-ba17-07eae31aea35",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from helper import load_mistral_api_key\n",
    "api_key, dlai_endpoint = load_mistral_api_key(ret_key=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ce5f6-5eb1-4442-8e04-822bdbd701f4",
   "metadata": {},
   "source": [
    "### Parse the article with BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4c01740-72b4-482c-b61e-e272a734f01f",
   "metadata": {
    "height": 217
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can AI help to fight climate change? A new report evaluates progress so far and explores options for the future.What’s new: The Innovation for Cool Earth Forum, a conference of climate researchers hosted by Japan, published a roadmap for the use of data science, computer vision, and AI-driven simulation to reduce greenhouse gas emissions. The roadmap evaluates existing approaches and suggests ways to scale them up.How it works: The roadmap identifies 6 “high-potential opportunities”: activities in which AI systems can make a significant difference based on the size of the opportunity, real-world results, and validated research. The authors emphasize the need for data, technical and scientific talent, computing power, funding, and leadership to take advantage of these opportunities.Monitoring emissions. AI systems analyze data from satellites, drones, and ground sensors to measure greenhouse gas emissions. The European Union uses them to measure methane emissions, environmental organizations gauge carbon monoxide emissions to help guide the carbon offset trading market, and consultancies like Kayrros identify large-scale sources of greenhouse gasses like landfills and oilfields. The authors recommend an impartial clearinghouse for climate-related data and wider access to satellite data.Energy. More than 30 percent of carbon emissions come from generating electricity. Simulations based on neural networks are helping to predict power generated by wind and solar plants and demand on electrical grids, which have proven to be difficult for other sorts of algorithms. AI systems also help to situate wind and solar plants and optimize grids. These approaches could scale up with more robust models, standards to evaluate performance, and security protocols.Manufacturing. An unnamed Brazilian steelmaker has used AI to measure the chemical composition of scrap metal to be reused batch by batch, allowing it to reduce carbon-intensive additives by 8 percent while improving overall quality. AI systems can analyze historical data to help factories use more recycled materials, cut waste, minimize energy use, and reduce downtime. Similarly, they can optimize supply chains to reduce emissions contributed by logistics. Agriculture. Farmers use AI-equipped sensors to simulate different crop rotations and weather events to forecast crop yield or loss. Armed with this data, food producers can cut waste and reduce carbon footprints. The authors cite lack of food-related datasets and investment in adapting farming practices as primary barriers to taking full advantage of AI in the food industry.Transportation. AI systems can reduce greenhouse-gas emissions by improving traffic flow, ameliorating congestion, and optimizing public transportation. Moreover, reinforcement learning can reduce the impact of electric vehicles on the power grid by optimizing their charging. More data, uniform standards, and AI talent are needed to realize this potential.Materials. Materials scientists use AI models to study traits of existing materials and design new ones. These techniques could accelerate development of more efficient batteries, solar cells, wind turbines, and transmission infrastructure. Better coordination between materials scientists and AI researchers would accelerate such benefits.Why it matters: AI has demonstrated its value in identifying sources of emissions, optimizing energy consumption, and developing and understanding materials. Scaling and extending this value in areas that generate the most greenhouse gasses — particularly energy generation, manufacturing, food production, and transportation — could make a significant dent in greenhouse gas emissions.We’re thinking: AI also has an important role to play in advancing the science of climate geoengineering, such as stratospheric aerosol injection (SAI), to cool down the planet. More research is needed to determine whether SAI is a good idea, but AI-enabled climate modeling will help answer this question.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "response = requests.get(\n",
    "    \"https://www.deeplearning.ai/the-batch/a-roadmap-explores-how-ai-can-detect-and-mitigate-greenhouse-gases/\"\n",
    ")\n",
    "html_doc = response.text\n",
    "soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "tag = soup.find(\"div\", re.compile(\"^prose--styled\"))\n",
    "text = tag.text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fbfa8e2-08af-445b-8134-7395cc693b5b",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "file_name = \"AI_greenhouse_gas.txt\"\n",
    "with open(file_name, 'w') as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad1aa61-9e1c-46c8-ae5e-61855df440f9",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "Let's split the document into chunks. It is crucial to do so in a RAG system to be able to more effectively identify and retrieve the most relevant piece of information. Here we will simple split the text by character combining 512 characters into each chunk. Depending on our specific use cases it may be necessary to customize or experiment with different chunk sizes. Also there are various ootions in terms of how we split the text. We can split by tokens, sentences, HTML headers and others depending on our application. After this we have to make embeddings for each of these chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8494655e-bd87-49de-8f1d-69ffbc1c256e",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "chunk_size = 512\n",
    "chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c78c9936-0c1d-471c-b030-6c45639e7238",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e3f06-09d6-4186-be0b-6034b0c8330e",
   "metadata": {},
   "source": [
    "### Get embeddings of the chunks\n",
    "We define the get_text_embedding() function using the Mistral embeddings API endpoint to get embedding from a single text chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e77d9805-7a53-4210-9f80-f4de52285588",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai.client import MistralClient\n",
    "\n",
    "\n",
    "def get_text_embedding(txt):\n",
    "    client = MistralClient(api_key=api_key, endpoint=dlai_endpoint)\n",
    "    embeddings_batch_response = client.embeddings(model=\"mistral-embed\", input=txt)\n",
    "    return embeddings_batch_response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46503830-6ad5-493e-a629-152721e2d88e",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55396758-c3f3-45b3-b6e7-d4912c0899f2",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03274536,  0.04751587,  0.04489136, ..., -0.03289795,\n",
       "         0.02278137, -0.01459503],\n",
       "       [-0.03631592,  0.05548096,  0.03271484, ..., -0.03125   ,\n",
       "         0.01594543, -0.01722717],\n",
       "       [-0.04876709,  0.04779053,  0.05670166, ...,  0.0046463 ,\n",
       "         0.0184021 , -0.01251984],\n",
       "       ...,\n",
       "       [-0.02597046,  0.04049683,  0.03543091, ..., -0.01013184,\n",
       "        -0.00962067, -0.00917053],\n",
       "       [-0.03025818,  0.0541687 ,  0.06280518, ..., -0.00900269,\n",
       "        -0.00782776, -0.00432587],\n",
       "       [-0.02456665,  0.05093384,  0.04879761, ..., -0.0064888 ,\n",
       "         0.02600098, -0.01386261]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca875993-fe6d-42df-811e-a43891cd0350",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba33c7-9d1d-44d8-a01e-e30f16be1aac",
   "metadata": {},
   "source": [
    "### Store in a vector databsae\n",
    "Then we use the list comprehension to get text embeddings for all text chunks. The embedding are of 1024 dimension. [Faiss](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0981a956-5f9c-4ea6-8fb3-a2cc9fe896d2",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee023ab-b26c-4df5-8a7b-7dd660bfad86",
   "metadata": {},
   "source": [
    "### Embed the user query\n",
    "For storing the embeddings in vector database we will use Faiss library, this is a common practice to store the embeddings for efficient processing and retrieval. With Faiss we define an instance of index class with the embedding dimension as the argument. We then add the text embeddings to the indexing structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "894d9764-9da9-4629-8f2a-c9dcaf6ceb8d",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "question = \"What are the ways that AI can reduce emissions in Agriculture?\"\n",
    "question_embeddings = np.array([get_text_embedding(question)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c4948cc-6d8b-449f-bc00-abb3591c7222",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00073624,  0.04116821,  0.04318237, ..., -0.02453613,\n",
       "         0.01029968,  0.00930023]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15989e10-d0ec-41be-b6be-fa317565a926",
   "metadata": {},
   "source": [
    "### Search for chunks that are similar to the query\n",
    "When user asks question, we also need to create embeddings for this question using the same embedding model as before. Now we can retireve text chunks from the vector database that's similar to the question we asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c930b378-7aac-434c-881b-ab69d3edb93d",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 5]]\n"
     ]
    }
   ],
   "source": [
    "D, I = index.search(question_embeddings, k=2)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1602e1b9-df27-4cce-a13d-cc9035669442",
   "metadata": {},
   "source": [
    "We can perform a search on the vector database with index.search. This function returns the distances and the indices of the k most similar vectors to the question vector in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73aab584-1dbf-4532-b41e-0403eeeeb567",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data to help factories use more recycled materials, cut waste, minimize energy use, and reduce downtime. Similarly, they can optimize supply chains to reduce emissions contributed by logistics.\\xa0Agriculture.\\xa0Farmers use AI-equipped sensors to simulate different crop rotations and weather events to forecast crop yield or loss. Armed with this data, food producers can cut waste and reduce carbon footprints. The authors cite lack of food-related datasets and investment in adapting farming practices as primary b', 'arriers to taking full advantage of AI in the food industry.Transportation.\\xa0AI systems can reduce greenhouse-gas emissions by improving traffic flow, ameliorating congestion, and optimizing public transportation. Moreover, reinforcement learning can reduce the impact of electric vehicles on the power grid by optimizing their charging. More data, uniform standards, and AI talent are needed to realize this potential.Materials.\\xa0Materials scientists use AI models to study traits of existing materials and design']\n"
     ]
    }
   ],
   "source": [
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "print(retrieved_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015b8d6d-5788-4a68-a5e7-48a315ba2d58",
   "metadata": {},
   "source": [
    "Then based on the return indices, we can retrieve the actual relevant text chunks that correspond to those indices. In the response we get two text chunks because we defined k=2 to retireve the two most similar vectors in the vector database.\n",
    "There are lot of different retireval strategies. Here we used a simple similarity search with embeddings. Depending on our use case sometimes we might want to perform metadata filtering first, or provide weights to the retrieved documents, or retrieve a larger parent child that original retrieved chunks belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da042a53-4564-4057-9a60-9b57dffff6a1",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51702d25-c454-4f2b-b678-3ae527c8f3c0",
   "metadata": {},
   "source": [
    "Finally we can offer the retrieved text chunks as the context information within the prompt. In the prompt template we can include both the retrieved text chunks and the user question in a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94e7661e-51e2-4148-a44c-f262e7e85d56",
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "\n",
    "def mistral(user_message, model=\"mistral-small-latest\", is_json=False):\n",
    "    client = MistralClient(api_key=api_key, endpoint=dlai_endpoint)\n",
    "    messages = [ChatMessage(role=\"user\", content=user_message)]\n",
    "\n",
    "    if is_json:\n",
    "        chat_response = client.chat(\n",
    "            model=model, messages=messages, response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "    else:\n",
    "        chat_response = client.chat(model=model, messages=messages)\n",
    "\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46a964d3-0dea-422a-83e6-342a4ab6906b",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context provided, AI can reduce emissions in agriculture by helping farmers use AI-equipped sensors to simulate different crop rotations and weather events. This allows them to forecast crop yield or loss, which in turn enables food producers to cut waste and reduce their carbon footprints.\n"
     ]
    }
   ],
   "source": [
    "response = mistral(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe48f5a-94c4-41c3-824e-4213320784c6",
   "metadata": {
    "height": 30
   },
   "source": [
    "With the prompt we get a response. And this is how RAG works from scratch.\n",
    "\n",
    "If we are developing a complex application where RAG is one of the tools we can call, or if we have multiple RAGs as multiple tools we can call, then we may consider using RAG in setup function calling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a653b9c2-d6e7-42f5-88e9-d5dcd376e61e",
   "metadata": {},
   "source": [
    "## RAG + Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f41aac3a-20b4-4e33-ac58-f245577141f8",
   "metadata": {
    "height": 455
   },
   "outputs": [],
   "source": [
    "def qa_with_context(text, question, chunk_size=512):\n",
    "    # split document into chunks\n",
    "    chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    # load into a vector database\n",
    "    text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n",
    "    d = text_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(text_embeddings)\n",
    "    # create embeddings for a question\n",
    "    question_embeddings = np.array([get_text_embedding(question)])\n",
    "    # retrieve similar chunks from the vector database\n",
    "    D, I = index.search(question_embeddings, k=2)\n",
    "    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "    # generate response based on the retrieve relevant text chunks\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Context information is below.\n",
    "    ---------------------\n",
    "    {retrieved_chunk}\n",
    "    ---------------------\n",
    "    Given the context information and not prior knowledge, answer the query.\n",
    "    Query: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    response = mistral(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddb4467f-0db8-4247-8150-8746a4630432",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 5]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b1bcc8d-b957-4167-b1e9-1353a6301762",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17142ead-beb7-4ff3-874f-069306565542",
   "metadata": {},
   "source": [
    "The above function is a wrap up of the RAG knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f23d8ef9-36d4-4912-8303-d2fe3860d7c6",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "names_to_functions = {\"qa_with_context\": functools.partial(qa_with_context, text=text)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e3fef5-9e44-4480-8a2d-44c218d6921f",
   "metadata": {},
   "source": [
    "Then we organize this function into a dictionary, this might not look that useful with just one function, but if we have multiple tools or functions, this is very useful to organize them into one dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cae3717b-37e6-40b3-93b1-cfd023b59079",
   "metadata": {
    "height": 336
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"qa_with_context\",\n",
    "            \"description\": \"Answer user question by retrieving relevant context\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"question\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"user question\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"question\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14657916-152e-4f08-9b15-c4b0ae08176c",
   "metadata": {},
   "source": [
    "We can outline the function specs with a JSON schema to tell the model what this function is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2e442fa-5cca-4eb1-9c3f-24276fe4f75c",
   "metadata": {
    "height": 251
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(id='64d6c8310b164881bad2e670dd01138b', object='chat.completion', created=1715972374, model='mistral-large-latest', choices=[ChatCompletionResponseChoice(index=0, message=ChatMessage(role='assistant', content='', name=None, tool_calls=[ToolCall(id='Ceah0Qorr', type=<ToolType.function: 'function'>, function=FunctionCall(name='qa_with_context', arguments='{\"question\": \"What are the ways AI can mitigate climate change in transportation?\"}'))]), finish_reason=<FinishReason.tool_calls: 'tool_calls'>)], usage=UsageInfo(prompt_tokens=92, total_tokens=126, completion_tokens=34))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "What are the ways AI can mitigate climate change in transportation?\n",
    "\"\"\"\n",
    "\n",
    "client = MistralClient(api_key=api_key, endpoint=dlai_endpoint)\n",
    "\n",
    "response = client.chat(\n",
    "    model=\"mistral-large-latest\",\n",
    "    messages=[ChatMessage(role=\"user\", content=question)],\n",
    "    tools=tools,\n",
    "    tool_choice=\"any\",\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d349dd7-0138-4857-9bcb-69ed151cb1b8",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(name='qa_with_context', arguments='{\"question\": \"What are the ways AI can mitigate climate change in transportation?\"}')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_function = response.choices[0].message.tool_calls[0].function\n",
    "tool_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35437438-1654-43ea-aa75-6580110d6730",
   "metadata": {},
   "source": [
    "Now we pass the user question and the tool to the model, we get two call results with the function name and the arguments in our user question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca751c08-e6e7-46a4-8e4c-a30407853cfc",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qa_with_context'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_function.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08910b72-2aaa-4393-a35a-5ed2671b8faf",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the ways AI can mitigate climate change in transportation?'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "args = json.loads(tool_function.arguments)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "409f6a67-2787-424e-8b8d-92fc9b66bdf9",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The context information does not provide specific details on how AI can mitigate climate change in transportation. However, it does mention that AI has a role to play in reducing greenhouse gas emissions in various sectors including manufacturing, food production, and transportation. According to the roadmap published by the Innovation for Cool Earth Forum, there are six high-potential opportunities for AI to help reduce greenhouse gas emissions, but it does not specify which of these opportunities pertain specifically to the transportation sector.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_result = names_to_functions[tool_function.name](**args)\n",
    "function_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2d1982-a899-4ad5-a5de-2a33d46cd311",
   "metadata": {},
   "source": [
    "## More about RAG\n",
    "To learn about more advanced chunking and retrieval methods, you can check out:\n",
    "- [Advanced Retrieval for AI with Chroma](https://learn.deeplearning.ai/courses/advanced-retrieval-for-ai/lesson/1/introduction)\n",
    "  - Sentence window retrieval\n",
    "  - Auto-merge retrieval\n",
    "- [Building and Evaluating Advanced RAG Applications](https://learn.deeplearning.ai/courses/building-evaluating-advanced-rag)\n",
    "  - Query Expansion\n",
    "  - Cross-encoder reranking\n",
    "  - Training and utilizing Embedding Adapters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9932106-163e-45f4-85db-d6b373cf5bbd",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
